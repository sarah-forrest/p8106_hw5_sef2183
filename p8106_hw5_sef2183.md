P8106 Data Science II Homework 5
================
Sarah Forrest - sef2183
5/5/2023

# 1. Predicting gas milage using the auto dataset

In this problem, we will apply support vector machines to predict
whether a given car gets high or low gas mileage based on the dataset
“auto.csv”. The dataset contains 392 observations. The response variable
is `mpg_cat`, which is a binary variable that indicates whether the
miles per gallon of a car is high or low. The predictors are
`cylinders`, `displacement`, `horsepower`, `weight`, `acceleration`,
`year`, and `origin`.

``` r
# read in data
auto = read.csv("data/auto.csv") 
```

Set the `mpg_cat` variable to a factor.

``` r
auto$mpg_cat <- factor(auto$mpg_cat, c("high", "low"))
```

Create dummy variables for `origin` (1 = American, 2 = European, 3 =
Japanese) so it will be treated as a character variable rather than a
numeric variable. Two dummy variables are created: one for American cars
(1 = American, 0 = otherwise) and one for European cars (1 = European, 0
= otherwise). Note that cars with Japanese origin have a value of 0 for
both `origin_american` and `origin_european` dummy variables.

``` r
auto$origin_american <- ifelse(auto$origin == 1, 1, 0) # dummy variable for american origin (origin = 1)
auto$origin_european <- ifelse(auto$origin == 2, 1, 0) # dummy variable for european origin (origin = 2)

# remove original origin variable
auto$origin <- NULL
```

Split the dataset into two parts: training data (70%) and test data
(30%)

``` r
set.seed(1) # for reproducibility

# specify rows of training data (70% of the dataset)
rowTrain <- createDataPartition(y = auto$mpg_cat, 
                              p = .7,
                              list = F)

# create training dataset
auto_train <- auto[rowTrain, ]

# create test dataset
auto_test <- auto[-rowTrain, ]
```

## (a) Fit a support vector classifier (linear kernel) to the training data.

**Linear Boundary**

``` r
set.seed(1)

# tuning parameter cost for linear boundary
linear.tune <- tune.svm(mpg_cat ~ . ,
                        data = auto_train,
                        kernel = "linear",
                        cost = exp(seq(-5,2,len = 50)), # specify a grid of cost parameters with a length of 50
                        scale = TRUE) # must scale predictors when running svm model
plot(linear.tune)
```

![](p8106_hw5_sef2183_files/figure-gfm/unnamed-chunk-6-1.png)<!-- -->

``` r
# summary(linear.tune)
linear.tune$best.parameters
##         cost
## 33 0.6514391
```

The optimal value for the cost tuning parameter is 0.6514391.

**Fit optimal support vector classifier (linear kernel) using the best
cost parameter**

``` r
svm_model_lin <- linear.tune$best.model

# print the model summary
summary(svm_model_lin)
## 
## Call:
## best.svm(x = mpg_cat ~ ., data = auto_train, cost = exp(seq(-5, 2, 
##     len = 50)), kernel = "linear", scale = TRUE)
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  linear 
##        cost:  0.6514391 
## 
## Number of Support Vectors:  64
## 
##  ( 31 33 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  high low
```

### Training error rate

``` r
# predict the support vector classifier (linear kernel) on the training data
train_pred_lin <- predict(svm_model_lin, newdata = auto_train)

# confusion matrix
confusionMatrix(data = train_pred_lin,
                reference = auto_train$mpg_cat)
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction high low
##       high  132  13
##       low     6 125
##                                          
##                Accuracy : 0.9312         
##                  95% CI : (0.8946, 0.958)
##     No Information Rate : 0.5            
##     P-Value [Acc > NIR] : <2e-16         
##                                          
##                   Kappa : 0.8623         
##                                          
##  Mcnemar's Test P-Value : 0.1687         
##                                          
##             Sensitivity : 0.9565         
##             Specificity : 0.9058         
##          Pos Pred Value : 0.9103         
##          Neg Pred Value : 0.9542         
##              Prevalence : 0.5000         
##          Detection Rate : 0.4783         
##    Detection Prevalence : 0.5254         
##       Balanced Accuracy : 0.9312         
##                                          
##        'Positive' Class : high           
## 

# compute the training error rate
train_error_rate_lin <- mean(train_pred_lin != auto_train$mpg_cat)
train_error_rate_lin
## [1] 0.06884058
```

Error rate is calculated as the total number of two incorrect
predictions (FN + FP) divided by the total number of a dataset (N).
Therefore, the training error rate = (6 + 13) / 276 = 0.0688. This is
also equivalent to 1 minus the accuracy = 1 - 0.9312 = **0.0688**

### Test error rate

``` r
# predict the support vector classifier (linear kernel) on the test data
test_pred_lin <- predict(svm_model_lin, newdata = auto_test)

# confusion matrix
confusionMatrix(data = test_pred_lin,
                reference = auto_test$mpg_cat)
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction high low
##       high   53   9
##       low     5  49
##                                           
##                Accuracy : 0.8793          
##                  95% CI : (0.8058, 0.9324)
##     No Information Rate : 0.5             
##     P-Value [Acc > NIR] : <2e-16          
##                                           
##                   Kappa : 0.7586          
##                                           
##  Mcnemar's Test P-Value : 0.4227          
##                                           
##             Sensitivity : 0.9138          
##             Specificity : 0.8448          
##          Pos Pred Value : 0.8548          
##          Neg Pred Value : 0.9074          
##              Prevalence : 0.5000          
##          Detection Rate : 0.4569          
##    Detection Prevalence : 0.5345          
##       Balanced Accuracy : 0.8793          
##                                           
##        'Positive' Class : high            
## 

# compute the test error rate
test_error_rate_lin <- mean(test_pred_lin != auto_test$mpg_cat)
test_error_rate_lin
## [1] 0.1206897
```

The test error rate = (5 + 9) / 116 = 0.1207. This is also equivalent to
1 minus the accuracy = 1 - 0.8793 = **0.1207**

# (b) Fit a support vector machine with a radial kernel to the training data.

**Non-Linear Boundary**

``` r
set.seed(1)

# tuning parameter cost and gamma
radial.tune <- tune.svm(mpg_cat ~ . ,
                        data = auto_train,
                        kernel = "radial",
                        cost = exp(seq(1,7,len = 50)), # specify a grid of cost parameters with a length of 50
                        gamma = exp(seq(-10,-2,len = 20))) # specify a grid of gamma parameters with a length of 20

plot(radial.tune, transform.y = log, transform.x = log,
     color.palette = terrain.colors)
```

![](p8106_hw5_sef2183_files/figure-gfm/unnamed-chunk-10-1.png)<!-- -->

``` r
# summary(radial.tune)
radial.tune$best.parameters
##          gamma     cost
## 717 0.03826736 197.4952
```

The optimal value for the cost tuning parameter is 197.4952 and the
optimal value for the gamma tuning parameter is 0..03826736.

**Fit optimal support vector classifier (radial kernel) using the best
parameters**

``` r
svm_model_rad <- radial.tune$best.model

# print the model summary
summary(svm_model_rad)
## 
## Call:
## best.svm(x = mpg_cat ~ ., data = auto_train, gamma = exp(seq(-10, 
##     -2, len = 20)), cost = exp(seq(1, 7, len = 50)), kernel = "radial")
## 
## 
## Parameters:
##    SVM-Type:  C-classification 
##  SVM-Kernel:  radial 
##        cost:  197.4952 
## 
## Number of Support Vectors:  50
## 
##  ( 22 28 )
## 
## 
## Number of Classes:  2 
## 
## Levels: 
##  high low
```

### Training error rate

``` r
# predict the support vector machine (radial kernel) on the training data
train_pred_rad <- predict(svm_model_rad, newdata = auto_train)

# confusion matrix
confusionMatrix(data = train_pred_rad,
                reference = auto_train$mpg_cat)
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction high low
##       high  135   5
##       low     3 133
##                                           
##                Accuracy : 0.971           
##                  95% CI : (0.9437, 0.9874)
##     No Information Rate : 0.5             
##     P-Value [Acc > NIR] : <2e-16          
##                                           
##                   Kappa : 0.942           
##                                           
##  Mcnemar's Test P-Value : 0.7237          
##                                           
##             Sensitivity : 0.9783          
##             Specificity : 0.9638          
##          Pos Pred Value : 0.9643          
##          Neg Pred Value : 0.9779          
##              Prevalence : 0.5000          
##          Detection Rate : 0.4891          
##    Detection Prevalence : 0.5072          
##       Balanced Accuracy : 0.9710          
##                                           
##        'Positive' Class : high            
## 

# compute the training error rate
train_error_rate_rad <- mean(train_pred_rad != auto_train$mpg_cat)
train_error_rate_rad
## [1] 0.02898551
```

The training error rate = (3 + 5) / 276 = 0.029. This is also equivalent
to 1 minus the accuracy = 1 - 0.971 = **0.029**

### Test error rate

``` r
# predict the support vector machine (radial kernel) on the test data
test_pred_rad <- predict(svm_model_rad, newdata = auto_test)

# confusion matrix
confusionMatrix(data = test_pred_rad,
                reference = auto_test$mpg_cat)
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction high low
##       high   49  10
##       low     9  48
##                                           
##                Accuracy : 0.8362          
##                  95% CI : (0.7561, 0.8984)
##     No Information Rate : 0.5             
##     P-Value [Acc > NIR] : 4.315e-14       
##                                           
##                   Kappa : 0.6724          
##                                           
##  Mcnemar's Test P-Value : 1               
##                                           
##             Sensitivity : 0.8448          
##             Specificity : 0.8276          
##          Pos Pred Value : 0.8305          
##          Neg Pred Value : 0.8421          
##              Prevalence : 0.5000          
##          Detection Rate : 0.4224          
##    Detection Prevalence : 0.5086          
##       Balanced Accuracy : 0.8362          
##                                           
##        'Positive' Class : high            
## 

# compute the test error rate
test_error_rate_rad <- mean(test_pred_rad != auto_test$mpg_cat)
test_error_rate_rad
## [1] 0.1637931
```

The test error rate = (9 + 10) / 116 = 0.1638. This is also equivalent
to 1 minus the accuracy = 1 - 0.8362 = **0.1638**

# 2. Hierarchical clustering on the states using the USArrests dataset

In this problem, we perform hierarchical clustering on the states using
the USArrests data in the ISLR package. For each of the 50 states in the
United States, the dataset contains the number of arrests per 100,000
residents for each of three crimes: Assault, Murder, and Rape. The
dataset also contains the percent of the population in each state living
in urban areas, UrbanPop. The four variables will be used as features
for clustering.

``` r
# read in data
arrests <- data.frame(USArrests)
```

## (a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states. Cut the dendrogram at a height that results in three distinct clusters.

``` r
set.seed(1)

# compute the euclidean distance matrix
dist_mat <- dist(arrests, method = "euclidean")

# perform hierarchical clustering with complete linkage
hc <- hclust(dist_mat, method = "complete") # complete linkage

# plot
plot(hc)
```

![](p8106_hw5_sef2183_files/figure-gfm/unnamed-chunk-15-1.png)<!-- -->

``` r
# cut the dendrogram at a height that results in three distinct clusters
hc_clusters <- cutree(hc, k = 3) # three clusters

# print the states in each cluster
cat("Cluster 1:", row.names(arrests[hc_clusters == 1,]))
## Cluster 1: Alabama Alaska Arizona California Delaware Florida Illinois Louisiana Maryland Michigan Mississippi Nevada New Mexico New York North Carolina South Carolina
cat("Cluster 2:", row.names(arrests[hc_clusters == 2,]))
## Cluster 2: Arkansas Colorado Georgia Massachusetts Missouri New Jersey Oklahoma Oregon Rhode Island Tennessee Texas Virginia Washington Wyoming
cat("Cluster 3:", row.names(arrests[hc_clusters == 3,]))
## Cluster 3: Connecticut Hawaii Idaho Indiana Iowa Kansas Kentucky Maine Minnesota Montana Nebraska New Hampshire North Dakota Ohio Pennsylvania South Dakota Utah Vermont West Virginia Wisconsin
```

**Cluster 1** The states in cluster 1 include: Alabama, Alaska, Arizona,
California, Delaware, Florida, Illinois, Louisiana, Maryland, Michigan,
Mississippi, Nevada, New Mexico, New York, North Carolina, and South
Carolina.

**Cluster 2** The states in cluster 2 include: Arkansas, Colorado,
Georgia, Massachusetts, Missouri, New Jersey, Oklahoma, Oregon, Rhode
Island, Tennessee, Texas, Virginia, Washington, and Wyoming.

**Cluster 3** The states in cluster 3 include: Connecticut, Hawaii,
Idaho, Indiana, Iowa, Kansas, Kentucky, Maine, Minnesota, Montana,
Nebraska, New Hampshire, North Dakota, Ohio, Pennsylvania, South Dakota,
Utah, Vermont, West Virginia, and Wisconsin.

## (b) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.

``` r
# scale the variables
arrests_scaled <- scale(arrests)

set.seed(1)

# compute the Euclidean distance matrix
dist_mat_scaled <- dist(arrests_scaled, method = "euclidean")

# perform hierarchical clustering with complete linkage
hc_scaled <- hclust(dist_mat_scaled, method = "complete") # complete linkage

# plot
plot(hc_scaled)
```

![](p8106_hw5_sef2183_files/figure-gfm/unnamed-chunk-16-1.png)<!-- -->

``` r
# cut the dendrogram at a height that results in three distinct clusters
hc_clusters_scaled <- cutree(hc_scaled, k = 3) # three clusters

# print the states in each cluster
cat("Cluster 1:", row.names(arrests[hc_clusters_scaled == 1,]))
## Cluster 1: Alabama Alaska Georgia Louisiana Mississippi North Carolina South Carolina Tennessee
cat("Cluster 2:", row.names(arrests[hc_clusters_scaled == 2,]))
## Cluster 2: Arizona California Colorado Florida Illinois Maryland Michigan Nevada New Mexico New York Texas
cat("Cluster 3:", row.names(arrests[hc_clusters_scaled == 3,]))
## Cluster 3: Arkansas Connecticut Delaware Hawaii Idaho Indiana Iowa Kansas Kentucky Maine Massachusetts Minnesota Missouri Montana Nebraska New Hampshire New Jersey North Dakota Ohio Oklahoma Oregon Pennsylvania Rhode Island South Dakota Utah Vermont Virginia Washington West Virginia Wisconsin Wyoming
```

**Cluster 1** The states in cluster 1 include: Alabama, Alaska, Georgia,
Louisiana, Mississippi, North Carolina, South Carolina, and Tennessee.

**Cluster 2** The states in cluster 2 include: Arizona, California,
Colorado, Florida, Illinois, Maryland, Michigan, Nevada, New Mexico, New
York, and Texas.

**Cluster 3** The states in cluster 3 include: Arkansas, Connecticut,
Delaware, Hawaii, Idaho, Indiana, Iowa, Kansas, Kentucky, Maine,
Massachusetts, Minnesota, Missouri, Montana, Nebraska, New Hampshire,
New Jersey, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode
Island, South Dakota, Utah, Vermont, Virginia, Washington, West
Virginia, Wisconsin, and Wyoming.

### Does scaling the variables change the clustering results?

Scaling the variables changed the clustering results. This can be due to
a change in the distances between the observations. It’s possible that
scaling the variables may lead to more meaningful clusters, especially
in scenarios where the variables have very different scales or units. In
this dataset, the `UrbanPop` variable (percent of the population in each
state living in urban areas) is at a different scale than the `Assault`,
`Murder`, and `Rape` variables (number of arrests per 100,000 residents
for each of the three crimes). This may be the reason why scaling the
variables changed the clustering results.

### Should the variables be scaled before the inter-observation dissimilarities are computed?

Variables should be scaled before inter-observation dissimilarities are
computed, especially if the variables have different scales or units.
Scaling helps to ensure that each variable contributes equally to the
distances between the observations. However, if the variables are
already on the same scale, then scaling may not be necessary or
required.
