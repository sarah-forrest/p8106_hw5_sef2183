---
title: "P8106 Data Science II Homework 5"
author: "Sarah Forrest - sef2183"
date: "5/5/2023"
output: github_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE, warning = FALSE, dpi = 300, fig.width = 7)
```

```{r, include = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(ISLR) # for the USArrests dataset
```

# 1. Predicting gas milage using the auto dataset

In this problem, we will apply support vector machines to predict whether a given car gets high or low gas mileage based on the dataset “auto.csv”. The dataset contains 392 observations. The response variable is `mpg_cat`, which indicates whether the miles per gallon of a car is high or low. The predictors are `cylinders`, `displacement`, `horsepower`, `weight`, `acceleration`, `year`, and `origin`. 

[NEED TO CREATE DUMMY VARIABLES FOR ORIGIN]

```{r}
# read in data
auto = read.csv("data/auto.csv") 
```

Split the dataset into two parts: training data (70%) and test data (30%).

```{r}
set.seed(1) # for reproducibility

# specify rows of training data (70% of the dataset)
rowTrain <- createDataPartition(y = auto$mpg_cat, 
                              p = .7,
                              list = F)
```

Mutate the data so the outcome variable `mpg_cat` takes numeric values of 0 and 1 rather than character values "low" and "high" [IS THIS NEEDED?]

```{r}
auto_glm = 
  auto %>%
  mutate(mpg_cat = case_when(
    mpg_cat == "low" ~ 0,
    mpg_cat == "high" ~ 1))
```
## (a) Fit a support vector classifier (linear kernel) to the training data. 

```{r}
```

### Training and test error rates

```{r}
```

# (b) Fit a support vector machine with a radial kernel to the training data.

```{r}
```

### Training and test error rates

```{r}
```

# 2. Hierarchical clustering on the states using the USArrests dataset

In this problem, we perform hierarchical clustering on the states using the USArrests data in the ISLR package. For each of the 50 states in the United States, the dataset contains the number of arrests per 100,000 residents for each of three crimes: Assault, Murder, and Rape. The dataset also contains the percent of the population in each state living in urban areas, UrbanPop. The four variables will be used as features for clustering.

```{r}
# read in data
arrests <- data.frame(USArrests)
```

## (a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states. Cut the dendrogram at a height that results in three distinct clusters. Which states belong to which clusters?

```{r}
```

## (b) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one. Does scaling the variables change the clustering results? Why? In your opinion, should the variables be scaled before the inter-observation dissimilarities are computed?

```{r}
```