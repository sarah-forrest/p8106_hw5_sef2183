---
title: "P8106 Data Science II Homework 5"
author: "Sarah Forrest - sef2183"
date: "5/5/2023"
output: github_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE, warning = FALSE, dpi = 300, fig.width = 7)
```

```{r, include = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(e1071)
library(ISLR) # for the USArrests dataset
```

# 1. Predicting gas milage using the auto dataset

In this problem, we will apply support vector machines to predict whether a given car gets high or low gas mileage based on the dataset “auto.csv”. The dataset contains 392 observations. The response variable is `mpg_cat`, which is a binary variable that indicates whether the miles per gallon of a car is high or low. The predictors are `cylinders`, `displacement`, `horsepower`, `weight`, `acceleration`, `year`, and `origin`. 

```{r}
# read in data
auto = read.csv("data/auto.csv") 
```

Set the `mpg_cat` variable to a factor.

```{r}
auto$mpg_cat <- factor(auto$mpg_cat, c("high", "low"))
```

Create dummy variables for `origin` (1 = American, 2 = European, 3 = Japanese) so it will be treated as a character variable rather than a numeric variable. Two dummy variables are created: one for American cars (1 = American, 0 = otherwise) and one for European cars (1 = European, 0 = otherwise). Note that cars with Japanese origin have a value of 0 for both `origin_american` and `origin_european` dummy variables.

```{r}
auto$origin_american <- ifelse(auto$origin == 1, 1, 0) # dummy variable for american origin (origin = 1)
auto$origin_european <- ifelse(auto$origin == 2, 1, 0) # dummy variable for european origin (origin = 2)

# remove original origin variable
auto$origin <- NULL
```

Split the dataset into two parts: training data (70%) and test data (30%)

```{r}
set.seed(1) # for reproducibility

# specify rows of training data (70% of the dataset)
rowTrain <- createDataPartition(y = auto$mpg_cat, 
                              p = .7,
                              list = F)

# create training dataset
auto_train <- auto[rowTrain, ]

# create test dataset
auto_test <- auto[-rowTrain, ]
```

## (a) Fit a support vector classifier (linear kernel) to the training data. 

**Linear Boundary**

```{r}
set.seed(1)

# tuning parameter cost for linear boundary
linear.tune <- tune.svm(mpg_cat ~ . ,
                        data = auto_train,
                        kernel = "linear",
                        cost = exp(seq(-5,2,len = 50)), # specify a grid of cost parameters with a length of 50
                        scale = TRUE) # must scale predictors when running svm model
plot(linear.tune)

# summary(linear.tune)
linear.tune$best.parameters
```

The optimal value for the cost tuning parameter is 0.6514391.

**Fit optimal support vector classifier (linear kernel) using the best cost parameter**

```{r}
svm_model_lin <- linear.tune$best.model

# print the model summary
summary(svm_model_lin)
```

### Training error rate

```{r}
# predict the support vector classifier (linear kernel) on the training data
train_pred_lin <- predict(svm_model_lin, newdata = auto_train)

# confusion matrix
confusionMatrix(data = train_pred_lin,
                reference = auto_train$mpg_cat)

# compute the training error rate
train_error_rate_lin <- mean(train_pred_lin != auto_train$mpg_cat)
train_error_rate_lin
```

Error rate is calculated as the total number of two incorrect predictions (FN + FP) divided by the total number of a dataset (N). Therefore, the training error rate = (6 + 13) / 276 = 0.0688. This is also equivalent to 1 minus the accuracy = 1 - 0.9312 = **0.0688**

### Test error rate

```{r}
# predict the support vector classifier (linear kernel) on the test data
test_pred_lin <- predict(svm_model_lin, newdata = auto_test)

# confusion matrix
confusionMatrix(data = test_pred_lin,
                reference = auto_test$mpg_cat)

# compute the test error rate
test_error_rate_lin <- mean(test_pred_lin != auto_test$mpg_cat)
test_error_rate_lin
```

The test error rate = (5 + 9) / 116 = 0.1207. This is also equivalent to 1 minus the accuracy = 1 - 0.8793 = **0.1207**

# (b) Fit a support vector machine with a radial kernel to the training data.

**Non-Linear Boundary**

```{r}
set.seed(1)

# tuning parameter cost and gamma
radial.tune <- tune.svm(mpg_cat ~ . ,
                        data = auto_train,
                        kernel = "radial",
                        cost = exp(seq(1,7,len = 50)), # specify a grid of cost parameters with a length of 50
                        gamma = exp(seq(-10,-2,len = 20))) # specify a grid of gamma parameters with a length of 20

plot(radial.tune, transform.y = log, transform.x = log,
     color.palette = terrain.colors)

# summary(radial.tune)
radial.tune$best.parameters
```

The optimal value for the cost tuning parameter is 197.4952 and the optimal value for the gamma tuning parameter is 0..03826736.

**Fit optimal support vector classifier (radial kernel) using the best parameters**

```{r}
svm_model_rad <- radial.tune$best.model

# print the model summary
summary(svm_model_rad)
```

### Training error rate

```{r}
# predict the support vector machine (radial kernel) on the training data
train_pred_rad <- predict(svm_model_rad, newdata = auto_train)

# confusion matrix
confusionMatrix(data = train_pred_rad,
                reference = auto_train$mpg_cat)

# compute the training error rate
train_error_rate_rad <- mean(train_pred_rad != auto_train$mpg_cat)
train_error_rate_rad
```

The training error rate = (3 + 5) / 276 = 0.029. This is also equivalent to 1 minus the accuracy = 1 - 0.971 = **0.029**

### Test error rate

```{r}
# predict the support vector machine (radial kernel) on the test data
test_pred_rad <- predict(svm_model_rad, newdata = auto_test)

# confusion matrix
confusionMatrix(data = test_pred_rad,
                reference = auto_test$mpg_cat)

# compute the test error rate
test_error_rate_rad <- mean(test_pred_rad != auto_test$mpg_cat)
test_error_rate_rad
```

The test error rate = (9 + 10) / 116 = 0.1638. This is also equivalent to 1 minus the accuracy = 1 - 0.8362 = **0.1638**

# 2. Hierarchical clustering on the states using the USArrests dataset

In this problem, we perform hierarchical clustering on the states using the USArrests data in the ISLR package. For each of the 50 states in the United States, the dataset contains the number of arrests per 100,000 residents for each of three crimes: Assault, Murder, and Rape. The dataset also contains the percent of the population in each state living in urban areas, UrbanPop. The four variables will be used as features for clustering.

```{r}
# read in data
arrests <- data.frame(USArrests)
```

## (a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states. Cut the dendrogram at a height that results in three distinct clusters.

```{r}
set.seed(1)

# compute the euclidean distance matrix
dist_mat <- dist(arrests, method = "euclidean")

# perform hierarchical clustering with complete linkage
hc <- hclust(dist_mat, method = "complete") # complete linkage

# plot
plot(hc)

# cut the dendrogram at a height that results in three distinct clusters
hc_clusters <- cutree(hc, k = 3) # three clusters

# print the states in each cluster
cat("Cluster 1:", row.names(arrests[hc_clusters == 1,]))
cat("Cluster 2:", row.names(arrests[hc_clusters == 2,]))
cat("Cluster 3:", row.names(arrests[hc_clusters == 3,]))
```

**Cluster 1**
The states in cluster 1 include: Alabama, Alaska, Arizona, California, Delaware, Florida, Illinois, Louisiana, Maryland, Michigan, Mississippi, Nevada, New Mexico, New York, North Carolina, and South Carolina.

**Cluster 2**
The states in cluster 2 include: Arkansas, Colorado, Georgia, Massachusetts, Missouri, New Jersey, Oklahoma, Oregon, Rhode Island, Tennessee, Texas, Virginia, Washington, and Wyoming.

**Cluster 3**
The states in cluster 3 include: Connecticut, Hawaii, Idaho, Indiana, Iowa, Kansas, Kentucky, Maine, Minnesota, Montana, Nebraska, New Hampshire, North Dakota, Ohio, Pennsylvania, South Dakota, Utah, Vermont, West Virginia, and Wisconsin.

## (b) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.

```{r}
# scale the variables
arrests_scaled <- scale(arrests)

set.seed(1)

# compute the Euclidean distance matrix
dist_mat_scaled <- dist(arrests_scaled, method = "euclidean")

# perform hierarchical clustering with complete linkage
hc_scaled <- hclust(dist_mat_scaled, method = "complete") # complete linkage

# plot
plot(hc_scaled)

# cut the dendrogram at a height that results in three distinct clusters
hc_clusters_scaled <- cutree(hc_scaled, k = 3) # three clusters

# print the states in each cluster
cat("Cluster 1:", row.names(arrests[hc_clusters_scaled == 1,]))
cat("Cluster 2:", row.names(arrests[hc_clusters_scaled == 2,]))
cat("Cluster 3:", row.names(arrests[hc_clusters_scaled == 3,]))
```

**Cluster 1**
The states in cluster 1 include: Alabama, Alaska, Georgia, Louisiana, Mississippi, North Carolina, South Carolina, and Tennessee.

**Cluster 2**
The states in cluster 2 include: Arizona, California, Colorado, Florida, Illinois, Maryland, Michigan, Nevada, New Mexico, New York, and Texas.

**Cluster 3**
The states in cluster 3 include: Arkansas, Connecticut, Delaware, Hawaii, Idaho, Indiana, Iowa, Kansas, Kentucky, Maine, Massachusetts, Minnesota, Missouri, Montana, Nebraska, New Hampshire, New Jersey, North Dakota, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Dakota, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, and Wyoming.

### Does scaling the variables change the clustering results? 

Scaling the variables changed the clustering results. This can be due to a change in the distances between the observations. It's possible that scaling the variables may lead to more meaningful clusters, especially in scenarios where the variables have very different scales or units. In this dataset, the `UrbanPop` variable (percent of the population in each state living in urban areas) is at a different scale than the `Assault`, `Murder`, and `Rape` variables (number of arrests per 100,000 residents for each of the three crimes). This may be the reason why scaling the variables changed the clustering results.

### Should the variables be scaled before the inter-observation dissimilarities are computed?

Variables should be scaled before inter-observation dissimilarities are computed, especially if the variables have different scales or units. Scaling helps to ensure that each variable contributes equally to the distances between the observations. However, if the variables are already on the same scale, then scaling may not be necessary or required.