---
title: "P8106 Data Science II Homework 5"
author: "Sarah Forrest - sef2183"
date: "5/5/2023"
output: github_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(collapse = TRUE, message = FALSE, warning = FALSE, dpi = 300, fig.width = 7)
```

```{r, include = FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(caret)
library(e1071)
library(ISLR) # for the USArrests dataset
library(factoextra)
```

# 1. Predicting gas milage using the auto dataset

In this problem, we will apply support vector machines to predict whether a given car gets high or low gas mileage based on the dataset “auto.csv”. The dataset contains 392 observations. The response variable is `mpg_cat`, which is a binary variable that indicates whether the miles per gallon of a car is high or low. The predictors are `cylinders`, `displacement`, `horsepower`, `weight`, `acceleration`, `year`, and `origin`. 

```{r}
# read in data
auto = read.csv("data/auto.csv") 
```

Set the `mpg_cat` variable to a factor.

```{r}
auto$mpg_cat <- factor(auto$mpg_cat, c("high", "low"))
```

Create dummy variables for `origin` (1 = American, 2 = European, 3 = Japanese) so it will be treated as a character variable rather than a numeric variable. Two dummy variables are created: one for American cars (1 = American, 0 = otherwise) and one for European cars (1 = European, 0 = otherwise). Note that cars with Japanese origin have a value of 0 for both `origin_american` and `origin_european` dummy variables.

```{r}
auto$origin_american <- ifelse(auto$origin == 1, 1, 0) # dummy variable for american origin (origin = 1)
auto$origin_european <- ifelse(auto$origin == 2, 1, 0) # dummy variable for european origin (origin = 2)

# remove original origin variable
auto$origin <- NULL
```

Split the dataset into two parts: training data (70%) and test data (30%)

```{r}
set.seed(1) # for reproducibility

# specify rows of training data (70% of the dataset)
rowTrain <- createDataPartition(y = auto$mpg_cat, 
                              p = .7,
                              list = F)

# create training dataset
auto_train <- auto[rowTrain, ]

# create test dataset
auto_test <- auto[-rowTrain, ]
```

## (a) Fit a support vector classifier (linear kernel) to the training data. 

**Linear Boundary**

```{r}
set.seed(1)

# tuning parameter cost for linear boundary
linear.tune <- tune.svm(mpg_cat ~ . ,
                        data = auto_train,
                        kernel = "linear",
                        cost = exp(seq(-5,2,len = 50)), # specify a grid of cost parameters with a length of 50
                        scale = TRUE) # must scale predictors when running svm model
plot(linear.tune)

# summary(linear.tune)
linear.tune$best.parameters
```

The optimal value for the cost tuning parameter is 0.6514391.

**Fit optimal support vector classifier (linear kernel) using the best cost parameter**

```{r}
svm_model_lin <- linear.tune$best.model

# print the model summary
summary(svm_model_lin)
```

### Training error rate

```{r}
# predict the support vector classifier (linear kernel) on the training data
train_pred_lin <- predict(svm_model_lin, newdata = auto_train)

# confusion matrix
confusionMatrix(data = train_pred_lin,
                reference = auto_train$mpg_cat)

# compute the training error rate
train_error_rate_lin <- mean(train_pred_lin != auto_train$mpg_cat)
train_error_rate_lin
```

Error rate is calculated as the total number of two incorrect predictions (FN + FP) divided by the total number of a dataset (N). Therefore, the training error rate = (6 + 13) / 276 = 0.0688. This is also equivalent to 1 minus the accuracy = 1 - 0.9312 = **0.0688**

### Test error rate

```{r}
# predict the support vector classifier (linear kernel) on the test data
test_pred_lin <- predict(svm_model_lin, newdata = auto_test)

# confusion matrix
confusionMatrix(data = test_pred_lin,
                reference = auto_test$mpg_cat)

# compute the test error rate
test_error_rate_lin <- mean(test_pred_lin != auto_test$mpg_cat)
test_error_rate_lin
```

The test error rate = (5 + 9) / 116 = 0.1207. This is also equivalent to 1 minus the accuracy = 1 - 0.8793 = **0.1207**

# (b) Fit a support vector machine with a radial kernel to the training data.

**Non-Linear Boundary**

```{r}
set.seed(1)

# tuning parameter cost and gamma
radial.tune <- tune.svm(mpg_cat ~ . ,
                        data = auto_train,
                        kernel = "radial",
                        cost = exp(seq(1,7,len = 50)), # specify a grid of cost parameters with a length of 50
                        gamma = exp(seq(-10,-2,len = 20))) # specify a grid of gamma parameters with a length of 20

plot(radial.tune, transform.y = log, transform.x = log,
     color.palette = terrain.colors)

# summary(radial.tune)
radial.tune$best.parameters
```

The optimal value for the cost tuning parameter is 197.4952 and the optimal value for the gamma tuning parameter is 0..03826736.

**Fit optimal support vector classifier (radial kernel) using the best parameters**

```{r}
svm_model_rad <- radial.tune$best.model

# print the model summary
summary(svm_model_rad)
```

### Training error rate

```{r}
# predict the support vector machine (radial kernel) on the training data
train_pred_rad <- predict(svm_model_rad, newdata = auto_train)

# confusion matrix
confusionMatrix(data = train_pred_rad,
                reference = auto_train$mpg_cat)

# compute the training error rate
train_error_rate_rad <- mean(train_pred_rad != auto_train$mpg_cat)
train_error_rate_rad
```

The training error rate = (3 + 5) / 276 = 0.029. This is also equivalent to 1 minus the accuracy = 1 - 0.971 = **0.029**

### Test error rate

```{r}
# predict the support vector machine (radial kernel) on the test data
test_pred_rad <- predict(svm_model_rad, newdata = auto_test)

# confusion matrix
confusionMatrix(data = test_pred_rad,
                reference = auto_test$mpg_cat)

# compute the test error rate
test_error_rate_rad <- mean(test_pred_rad != auto_test$mpg_cat)
test_error_rate_rad
```

The test error rate = (9 + 10) / 116 = 0.1638. This is also equivalent to 1 minus the accuracy = 1 - 0.8362 = **0.1638**

# 2. Hierarchical clustering on the states using the USArrests dataset

In this problem, we perform hierarchical clustering on the states using the USArrests data in the ISLR package. For each of the 50 states in the United States, the dataset contains the number of arrests per 100,000 residents for each of three crimes: Assault, Murder, and Rape. The dataset also contains the percent of the population in each state living in urban areas, UrbanPop. The four variables will be used as features for clustering and are scaled.

```{r}
# read in data
arrests <- data.frame(USArrests)
```

## (a) Using hierarchical clustering with complete linkage and Euclidean distance, cluster the states. Cut the dendrogram at a height that results in three distinct clusters.

Complete linkage and Euclidean distance is specified.

```{r}
hc.complete <- hclust(dist(arrests), method = "complete")
```

The function `fviz_dend()` is applied to visualize the dendrogram.

```{r}
 set.seed(1)

fviz_dend(hc.complete, k = 3, # 3 clusters
          cex = 0.3,
          palette = "jco",
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

# cut the dendrogram at a height that results in three distinct clusters
ind3.complete <- cutree(hc.complete, 3)
```

**Cluster 1**

```{r}
arrests[ind3.complete == 1,]
```

The states in cluster 1 include: Alabama, Alaska, Arizona, California, Delaware, Florida, Illinois, Louisiana, Maryland, Michigan, Mississippi, Nevada, New Mexico, New York, North Carolina, and South Carolina.

**Cluster 2**

```{r}
arrests[ind3.complete == 2,]
```

The states in cluster 2 include: Arkansas, Colorado, Georgia, Massachusetts, Missouri, New Jersey, Oklahoma, Oregon, Rhode Island, Tennessee, Texas, Virginia, Washington, Wyoming.

**Cluster 3**

```{r}
arrests[ind3.complete == 3,]
```

The states in cluster 3 include: Connecticut, Hawaii, Idaho, Indiana, Iowa, Kansas, Kentucky, Maine, Minnesota, Missouri, Nebraska, New Hampshire, North Dakota, Ohio, Pennsylvania, South Dakota, Utah, Vermont, West Virginia and Wisconsin.

## (b) Hierarchically cluster the states using complete linkage and Euclidean distance, after scaling the variables to have standard deviation one.

By default, the `scale()` function scales the data to have a mean of 0 and a standard deviation of 1.

```{r}
# scale the variables
arrests_scaled <- scale(arrests)
```

Complete linkage and Euclidean distance is specified.

```{r}
hc.complete_scaled <- hclust(dist(arrests_scaled), method = "complete")
```

The function `fviz_dend()` is applied to visualize the dendrogram.

```{r}
set.seed(1)

fviz_dend(hc.complete_scaled, k = 3, # 3 clusters
          cex = 0.3,
          palette = "jco",
          color_labels_by_k = TRUE,
          rect = TRUE, rect_fill = TRUE, rect_border = "jco",
          labels_track_height = 2.5)

# cut the dendrogram at a height that results in three distinct clusters
ind3.complete_scaled <- cutree(hc.complete_scaled, 3)
```

**Cluster 1**

```{r}
arrests[ind3.complete_scaled == 1,]
```

The states in cluster 1 include: Alabama, Alaska, Georgia, Louisiana, Mississippi, North Carolina, South Carolina, and Tennessee.

**Cluster 2**

```{r}
arrests[ind3.complete_scaled == 2,]
```

The states in cluster 2 include: Arizona, California, Colorado, Florida, Illinois, Maryland, Michigan, Nevada, New Mexico, New York, and Texas.

**Cluster 3**

```{r}
arrests[ind3.complete_scaled == 3,]
```

The states in cluster 3 include: Arkansas, Connecticut, Delaware, Hawaii, Idaho, Indiana, Iowa, Kansas, Kentucky, Maine, Minnesota, Missouri, Nebraska, New Hampshire, New Jersey, Ohio, Oklahoma, Oregon, Pennsylvania, Rhode Island, South Dakota, Utah, Vermont, Virginia, Washington, West Virginia, Wisconsin, and Wyoming.

### Does scaling the variables change the clustering results? 

**Scaling the variables changed the clustering results.** Cluster 3 for the scaled dataset is much larger than in the non-scaled dataset, and all clusters contain different states. This can be due to a change in the distances between the observations. Clustering algorithms typically use distance measures to group similar observations together.

It's possible that scaling the variables may lead to more meaningful clusters, especially in scenarios where the variables are measured on different scales or units. In this dataset, the `UrbanPop` variable (percent of the population in each state living in urban areas) is at a different scale than the `Assault`, `Murder`, and `Rape` variables (number of arrests per 100,000 residents for each of the three crimes). If the variables are measured on different scales, then the clustering algorithm may give more weight to the variable with the larger scale. This can result in misleading or biased clustering results. This may be the reason why scaling the variables changed the clustering results. By standardizing the variables, we put them on a common scale, which can help to avoid these problems.

### Should the variables be scaled before the inter-observation dissimilarities are computed?

**Variables should be scaled before inter-observation dissimilarities are computed**, especially if the variables are measured on different scales or units. Scaling helps to ensure that each variable contributes equally to the distances between the observations. Scaling the variables before computing inter-observation dissimilarities can help to ensure that the clustering analysis is not biased by differences in the scales of the variables. However, if the variables are already on the same scale, then scaling may not be necessary or required.